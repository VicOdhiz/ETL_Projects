
#import necessary libraries

import glob
import pandas as pd
from datetime import time
from datetime import datetime
import oracledb
import schedule
import time
import subprocess



#-----------------------------
#Load datasets from csv file
#EXTRACT

try:
    
    customers = pd.read_csv(r"C:\Users\Vic\Documents\Customer Dataset.csv")

    transactions = pd.read_csv(r"C:\Users\Vic\Documents\Transactions Dataset.csv")

    print('Data extracted successfully')

except FileNotFoundError as e:
    
    print('Error encountered during extraction, try again!')

    print ('Details:',e)


#------------------------------------
#TRANSFORM
#Join customers with transactions

customer_txn = pd.merge(transactions,customers, on = 'customer_id',how='inner')


#Aggregate:Total Spending per customer

try:
    
    customer_spending = customer_txn.groupby(["customer_id", "name", "country"], as_index=False)["amount"].sum()

# Rename column for clarity
    customer_spending.rename(columns={"amount": "total_spent"}, inplace=True)

except Exception as f:

    print('Data transformed successfully')

    print('Details:', f)


# --------------------------------------------------------------------------------------------

# LOAD

# ---------------------------------------------------------------------------------------------

# Save transformed data to new CSV

customer_spending.to_csv("customer_spending.csv", index=False)

print("Data Loaded Successfully into customer_spending.csv")


#-------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------
# LOGGING FUNCTION
def log_progress(message):

    timestamp_format =  '%d-%m-%Y %H:%M:%S' # Year-Month-Day Hour:Minute:Second

    now = datetime.now()

    timestamp = now.strftime(timestamp_format)

    with open("etl_log.txt", "a") as log_file:   # log to text file

        log_file.write(timestamp + ' - ' + message + '\n')

#--------------------------------------------------------
# Logging ETL progress
log_progress("ETL Job Started")

log_progress("Extract phase Completed")

log_progress("Transform phase Completed")

log_progress("Load phase Completed")

log_progress("ETL Job Ended")


#------------------------------------------------------------------------------------------------

#Staging the ETL logs in database


# Database connection details
dsn = oracledb.makedsn(

    host='localhost',

    port=1521,

    service_name='XEPDB1'
)
 
# Establish the connection
connection = oracledb.connect(

    user='system',

    password='xxxxxxxx',

    dsn=dsn
)
 
# Create a cursor to execute queries
cursor = connection.cursor()
 
# Define your query
create_table_query = """ BEGIN
   EXECUTE IMMEDIATE '
   CREATE TABLE etl_log (
       id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       job_name VARCHAR2(100),
       start_time TIMESTAMP,
       end_time TIMESTAMP,
       status VARCHAR2(20),
       rows_processed NUMBER
   )';
EXCEPTION
   WHEN OTHERS THEN
      IF SQLCODE = -955 THEN NULL; -- ORA-00955: name already used (skip if exists)
      ELSE RAISE;
      END IF;
END;
"""

#----------------------------------------------------------------
#Handle exceptions

try:
    cursor.execute(create_table_query)
    connection.commit()
    print("etl_log table created successfully")

except Exception as e:
    print("Table creation skipped or failed:", e)





# -----------------------------------------------------------------
# Insert ETL log

insert_query = """
    INSERT INTO etl_log (
    job_name, start_time, end_time, status, rows_processed)
    VALUES (:job_name, :start_time, :end_time, :status, :rows_processed)
"""

data = {
    "job_name": "ETL001 Job",
    "start_time": datetime.now(),
    "end_time": datetime.now(),
    "status": "Completed",
    "rows_processed": len(customer_spending) # replace with actual count
}

cursor.execute(insert_query, data)
connection.commit()
print("Log inserted successfully")
 
# --------------------------
# Fetch logs into DataFrame
df = pd.read_sql("SELECT * FROM etl_log", con=connection)

# Export to Excel
df.to_excel('etl_logs.xlsx', index=False)

print(df.shape)
print(df.head())

# Close connection
cursor.close()
connection.close()


#----------------------------------------------------------------------------

# Automatic scheduling of the job

def run_etl():

    subprocess.run(["python", "etl_job.py"])

# Run every hour
schedule.every().hour.do(run_etl)

while True:
    schedule.run_pending()
    time.sleep(60)

